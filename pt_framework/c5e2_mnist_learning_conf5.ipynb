{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FGcMmSn6HeyF",
        "outputId": "eb8618ef-b166-467d-a107-efd9ed0c5c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe MIT License (MIT)\\nCopyright (c) 2021 NVIDIA\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the \"Software\"), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\\nthe Software, and to permit persons to whom the Software is furnished to do so,\\nsubject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "The MIT License (MIT)\n",
        "Copyright (c) 2021 NVIDIA\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
        "this software and associated documentation files (the \"Software\"), to deal in\n",
        "the Software without restriction, including without limitation the rights to\n",
        "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
        "the Software, and to permit persons to whom the Software is furnished to do so,\n",
        "subject to the following conditions:\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
        "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
        "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
        "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
        "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0x7ACu7HeyG"
      },
      "source": [
        "This code example is very similar to c5e1_mnist_learning but the network is modified to use ReLU neruons in the hidden layer, softmax in the output layer, categorical crossentropy as loss function, Adam as optimizer, and a mini-batch size of 64. More context for this code example can be found in the section \"Experiment: Tweaking Network and Learning Parameters\" in Chapter 5 in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3INBBRP9HeyH",
        "outputId": "4e55fa72-35b6-4ae8-e22b-b4d2fa8cd9a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./pt_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 57380949.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./pt_data/MNIST/raw/train-images-idx3-ubyte.gz to ./pt_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./pt_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 58322433.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./pt_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./pt_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./pt_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 20874003.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./pt_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./pt_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./pt_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 10892240.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./pt_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./pt_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "torch.manual_seed(7)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Load training dataset into a single batch to compute mean and stddev.\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "trainset = MNIST(root='./pt_data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=True)\n",
        "data = next(iter(trainloader))\n",
        "mean = data[0].mean()\n",
        "stddev = data[0].std()\n",
        "\n",
        "# Helper function needed to standardize data when loading datasets.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(mean, stddev)])\n",
        "\n",
        "trainset = MNIST(root='./pt_data', train=True, download=True, transform=transform)\n",
        "testset = MNIST(root='./pt_data', train=False, download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC-XcacKHeyI"
      },
      "source": [
        "In this example we want to use ReLU as activation for the first layer and softmax for the second layer. One thing that is slightly unintuitive is that we omit the activation function altogether for the output layer. The reason is that the cross-entropy loss function implementation in PyTorch is implemented using the inputs to the softmax (also known as logits) instead of the outputs. This makes for an implementation that is more numerically stable (also see some more details in the section \"Computer implementation of the cross-entropy loss function\" in Chapter 5 in the book).\n",
        "\n",
        "Apart from changing the layer types, we also change the weight initialization scheme to Kaiming (He) for the ReLU layer and Xavier (Glorot) for the softmax layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "86xOALlzHeyI",
        "outputId": "d962b02b-b164-4766-829c-2720ecc01ea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Define model. Final activation is omitted since softmax is part of\n",
        "# cross-entropy loss function in PyTorch.\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(784, 25),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(25, 10)\n",
        ")\n",
        "\n",
        "# Retrieve layers for custom weight initialization.\n",
        "layers = next(model.modules())\n",
        "hidden_layer = layers[1]\n",
        "output_layer = layers[3]\n",
        "\n",
        "# Kaiming (He) initialization.\n",
        "nn.init.kaiming_normal_(hidden_layer.weight)\n",
        "nn.init.constant_(hidden_layer.bias, 0.0)\n",
        "\n",
        "# Xavier (Glorot) initialization.\n",
        "nn.init.xavier_uniform_(output_layer.weight)\n",
        "nn.init.constant_(output_layer.bias, 0.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYYKxRirHeyI"
      },
      "source": [
        "The training loop looks very similar to c5e1_mnist_learning. We change to using the Adam optimizer and the CrossEntropyLoss function.\n",
        "\n",
        "Additionally, inside the training loop we no longer conver the targets to one-hot encoded targets. The reason is that the CrossEntropyLoss function object is implemented in a way where it assumes that the input is an integer specifying the index to be hot instead of assuming a one-hot encoded input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wbNBPJ-bHeyI",
        "outputId": "169f9033-42af-4e58-d50f-bcf861c7f1ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 loss: 0.3651 - acc: 0.8902 - val_loss: 0.2167 - val_acc: 0.9310\n",
            "Epoch 2/20 loss: 0.1933 - acc: 0.9428 - val_loss: 0.1736 - val_acc: 0.9433\n",
            "Epoch 3/20 loss: 0.1576 - acc: 0.9527 - val_loss: 0.1529 - val_acc: 0.9501\n",
            "Epoch 4/20 loss: 0.1371 - acc: 0.9591 - val_loss: 0.1534 - val_acc: 0.9494\n",
            "Epoch 5/20 loss: 0.1246 - acc: 0.9621 - val_loss: 0.1349 - val_acc: 0.9565\n",
            "Epoch 6/20 loss: 0.1123 - acc: 0.9656 - val_loss: 0.1457 - val_acc: 0.9513\n",
            "Epoch 7/20 loss: 0.1052 - acc: 0.9679 - val_loss: 0.1300 - val_acc: 0.9555\n",
            "Epoch 8/20 loss: 0.0986 - acc: 0.9693 - val_loss: 0.1229 - val_acc: 0.9575\n",
            "Epoch 9/20 loss: 0.0915 - acc: 0.9715 - val_loss: 0.1294 - val_acc: 0.9557\n",
            "Epoch 10/20 loss: 0.0839 - acc: 0.9744 - val_loss: 0.1226 - val_acc: 0.9589\n",
            "Epoch 11/20 loss: 0.0816 - acc: 0.9738 - val_loss: 0.1321 - val_acc: 0.9569\n",
            "Epoch 12/20 loss: 0.0772 - acc: 0.9751 - val_loss: 0.1473 - val_acc: 0.9533\n",
            "Epoch 13/20 loss: 0.0731 - acc: 0.9760 - val_loss: 0.1206 - val_acc: 0.9602\n",
            "Epoch 14/20 loss: 0.0702 - acc: 0.9773 - val_loss: 0.1425 - val_acc: 0.9545\n",
            "Epoch 15/20 loss: 0.0671 - acc: 0.9785 - val_loss: 0.1245 - val_acc: 0.9603\n",
            "Epoch 16/20 loss: 0.0626 - acc: 0.9801 - val_loss: 0.1247 - val_acc: 0.9606\n",
            "Epoch 17/20 loss: 0.0611 - acc: 0.9804 - val_loss: 0.1366 - val_acc: 0.9586\n",
            "Epoch 18/20 loss: 0.0592 - acc: 0.9807 - val_loss: 0.1306 - val_acc: 0.9599\n",
            "Epoch 19/20 loss: 0.0573 - acc: 0.9816 - val_loss: 0.1295 - val_acc: 0.9594\n",
            "Epoch 20/20 loss: 0.0542 - acc: 0.9818 - val_loss: 0.1470 - val_acc: 0.9573\n"
          ]
        }
      ],
      "source": [
        "# Use the Adam optimizer\n",
        "# Cross-entropy loss as loss function.\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Transfer model to GPU\n",
        "model.to(device)\n",
        "\n",
        "# Create DataLoader objects that will help create mini-batches.\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Train the model. In PyTorch we have to implement the training loop ourselves.\n",
        "for i in range(EPOCHS):\n",
        "    model.train() # Set model in training mode.\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_batches = 0\n",
        "    for inputs, targets in trainloader:\n",
        "        # Move data to GPU.\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Zero the parameter gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass.\n",
        "        outputs = model(inputs)\n",
        "        # Cross-entropy loss does not need one-hot targets in PyTorch.\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        # Accumulate metrics.\n",
        "        _, indices = torch.max(outputs.data, 1)\n",
        "        train_correct += (indices == targets).sum().item()\n",
        "        train_batches +=  1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backward pass and update.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss = train_loss / train_batches\n",
        "    train_acc = train_correct / (train_batches * BATCH_SIZE)\n",
        "\n",
        "    # Evaluate the model on the test dataset. Identical to loop above but without\n",
        "    # weight adjustment.\n",
        "    model.eval() # Set model in inference mode.\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_batches = 0\n",
        "    for inputs, targets in testloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        _, indices = torch.max(outputs, 1)\n",
        "        test_correct += (indices == targets).sum().item()\n",
        "        test_batches +=  1\n",
        "        test_loss += loss.item()\n",
        "\n",
        "    test_loss = test_loss / test_batches\n",
        "    test_acc = test_correct / (test_batches * BATCH_SIZE)\n",
        "\n",
        "    print(f'Epoch {i+1}/{EPOCHS} loss: {train_loss:.4f} - acc: {train_acc:0.4f} - val_loss: {test_loss:.4f} - val_acc: {test_acc:0.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}